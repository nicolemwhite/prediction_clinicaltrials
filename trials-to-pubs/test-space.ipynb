{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<br><b>Beginning model application for NCT01346709</b><br>You may bookmark this page and return later.<br>\n",
      "<br><b>Beginning model application for NCT01346709</b><br>You may bookmark this page and return later.<br><b>Found 1 linked articles from PubMed, 1 result references from ClinicalTrials.gov, 1 unique PMIDs</b><br>Applying preselection, this will take about 60 seconds.<BR>\n",
      "<br><b>Beginning model application for NCT01346709</b><br>You may bookmark this page and return later.<br><b>Found 1 linked articles from PubMed, 1 result references from ClinicalTrials.gov, 1 unique PMIDs</b><br>Applying preselection, this will take about 60 seconds.<BR><b>5000 articles and linked articles have complete data on this server.</b><BR>\n",
      "<br><b>Beginning model application for NCT01346709</b><br>You may bookmark this page and return later.<br><b>Found 1 linked articles from PubMed, 1 result references from ClinicalTrials.gov, 1 unique PMIDs</b><br>Applying preselection, this will take about 60 seconds.<BR><b>5000 articles and linked articles have complete data on this server.</b><BR><b>Processing article titles and authors.</b><br>\n",
      "<br><b>Beginning model application for NCT01346709</b><br>You may bookmark this page and return later.<br><b>Found 1 linked articles from PubMed, 1 result references from ClinicalTrials.gov, 1 unique PMIDs</b><br>Applying preselection, this will take about 60 seconds.<BR><b>5000 articles and linked articles have complete data on this server.</b><BR><b>Processing article titles and authors.</b><br><b>Retrieving related article rankings from PubMed.gov</b><BR>\n",
      "<br><b>Beginning model application for NCT01346709</b><br>You may bookmark this page and return later.<br><b>Found 1 linked articles from PubMed, 1 result references from ClinicalTrials.gov, 1 unique PMIDs</b><br>Applying preselection, this will take about 60 seconds.<BR><b>5000 articles and linked articles have complete data on this server.</b><BR><b>Processing article titles and authors.</b><br><b>Retrieving related article rankings from PubMed.gov</b><BR><b>Retrieving <a href=\"http://arrowsmith.psych.uic.edu/arrowsmith_uic/author2.html\" target=\"_blank\">Author-ity</a> clusters.</b><br>\n",
      "<br><b>Beginning model application for NCT01346709</b><br>You may bookmark this page and return later.<br><b>Found 1 linked articles from PubMed, 1 result references from ClinicalTrials.gov, 1 unique PMIDs</b><br>Applying preselection, this will take about 60 seconds.<BR><b>5000 articles and linked articles have complete data on this server.</b><BR><b>Processing article titles and authors.</b><br><b>Retrieving related article rankings from PubMed.gov</b><BR><b>Retrieving <a href=\"http://arrowsmith.psych.uic.edu/arrowsmith_uic/author2.html\" target=\"_blank\">Author-ity</a> clusters.</b><br>500 Author-ity clusters searched out of 5000\n",
      "<br><b>Beginning model application for NCT01346709</b><br>You may bookmark this page and return later.<br><b>Found 1 linked articles from PubMed, 1 result references from ClinicalTrials.gov, 1 unique PMIDs</b><br>Applying preselection, this will take about 60 seconds.<BR><b>5000 articles and linked articles have complete data on this server.</b><BR><b>Processing article titles and authors.</b><br><b>Retrieving related article rankings from PubMed.gov</b><BR><b>Retrieving <a href=\"http://arrowsmith.psych.uic.edu/arrowsmith_uic/author2.html\" target=\"_blank\">Author-ity</a> clusters.</b><br>1000 Author-ity clusters searched out of 5000\n",
      "<br><b>Beginning model application for NCT01346709</b><br>You may bookmark this page and return later.<br><b>Found 1 linked articles from PubMed, 1 result references from ClinicalTrials.gov, 1 unique PMIDs</b><br>Applying preselection, this will take about 60 seconds.<BR><b>5000 articles and linked articles have complete data on this server.</b><BR><b>Processing article titles and authors.</b><br><b>Retrieving related article rankings from PubMed.gov</b><BR><b>Retrieving <a href=\"http://arrowsmith.psych.uic.edu/arrowsmith_uic/author2.html\" target=\"_blank\">Author-ity</a> clusters.</b><br>1500 Author-ity clusters searched out of 5000\n",
      "<br><b>Beginning model application for NCT01346709</b><br>You may bookmark this page and return later.<br><b>Found 1 linked articles from PubMed, 1 result references from ClinicalTrials.gov, 1 unique PMIDs</b><br>Applying preselection, this will take about 60 seconds.<BR><b>5000 articles and linked articles have complete data on this server.</b><BR><b>Processing article titles and authors.</b><br><b>Retrieving related article rankings from PubMed.gov</b><BR><b>Retrieving <a href=\"http://arrowsmith.psych.uic.edu/arrowsmith_uic/author2.html\" target=\"_blank\">Author-ity</a> clusters.</b><br>2000 Author-ity clusters searched out of 5000\n",
      "<br><b>Beginning model application for NCT01346709</b><br>You may bookmark this page and return later.<br><b>Found 1 linked articles from PubMed, 1 result references from ClinicalTrials.gov, 1 unique PMIDs</b><br>Applying preselection, this will take about 60 seconds.<BR><b>5000 articles and linked articles have complete data on this server.</b><BR><b>Processing article titles and authors.</b><br><b>Retrieving related article rankings from PubMed.gov</b><BR><b>Retrieving <a href=\"http://arrowsmith.psych.uic.edu/arrowsmith_uic/author2.html\" target=\"_blank\">Author-ity</a> clusters.</b><br>2500 Author-ity clusters searched out of 5000\n",
      "<br><b>Beginning model application for NCT01346709</b><br>You may bookmark this page and return later.<br><b>Found 1 linked articles from PubMed, 1 result references from ClinicalTrials.gov, 1 unique PMIDs</b><br>Applying preselection, this will take about 60 seconds.<BR><b>5000 articles and linked articles have complete data on this server.</b><BR><b>Processing article titles and authors.</b><br><b>Retrieving related article rankings from PubMed.gov</b><BR><b>Retrieving <a href=\"http://arrowsmith.psych.uic.edu/arrowsmith_uic/author2.html\" target=\"_blank\">Author-ity</a> clusters.</b><br>3000 Author-ity clusters searched out of 5000\n",
      "<br><b>Beginning model application for NCT01346709</b><br>You may bookmark this page and return later.<br><b>Found 1 linked articles from PubMed, 1 result references from ClinicalTrials.gov, 1 unique PMIDs</b><br>Applying preselection, this will take about 60 seconds.<BR><b>5000 articles and linked articles have complete data on this server.</b><BR><b>Processing article titles and authors.</b><br><b>Retrieving related article rankings from PubMed.gov</b><BR><b>Retrieving <a href=\"http://arrowsmith.psych.uic.edu/arrowsmith_uic/author2.html\" target=\"_blank\">Author-ity</a> clusters.</b><br>3500 Author-ity clusters searched out of 5000\n",
      "<br><b>Beginning model application for NCT01346709</b><br>You may bookmark this page and return later.<br><b>Found 1 linked articles from PubMed, 1 result references from ClinicalTrials.gov, 1 unique PMIDs</b><br>Applying preselection, this will take about 60 seconds.<BR><b>5000 articles and linked articles have complete data on this server.</b><BR><b>Processing article titles and authors.</b><br><b>Retrieving related article rankings from PubMed.gov</b><BR><b>Retrieving <a href=\"http://arrowsmith.psych.uic.edu/arrowsmith_uic/author2.html\" target=\"_blank\">Author-ity</a> clusters.</b><br>4000 Author-ity clusters searched out of 5000\n",
      "<br><b>Beginning model application for NCT01346709</b><br>You may bookmark this page and return later.<br><b>Found 1 linked articles from PubMed, 1 result references from ClinicalTrials.gov, 1 unique PMIDs</b><br>Applying preselection, this will take about 60 seconds.<BR><b>5000 articles and linked articles have complete data on this server.</b><BR><b>Processing article titles and authors.</b><br><b>Retrieving related article rankings from PubMed.gov</b><BR><b>Retrieving <a href=\"http://arrowsmith.psych.uic.edu/arrowsmith_uic/author2.html\" target=\"_blank\">Author-ity</a> clusters.</b><br>4500 Author-ity clusters searched out of 5000\n",
      "<br><b>Beginning model application for NCT01346709</b><br>You may bookmark this page and return later.<br><b>Found 1 linked articles from PubMed, 1 result references from ClinicalTrials.gov, 1 unique PMIDs</b><br>Applying preselection, this will take about 60 seconds.<BR><b>5000 articles and linked articles have complete data on this server.</b><BR><b>Processing article titles and authors.</b><br><b>Retrieving related article rankings from PubMed.gov</b><BR><b>Retrieving <a href=\"http://arrowsmith.psych.uic.edu/arrowsmith_uic/author2.html\" target=\"_blank\">Author-ity</a> clusters.</b><br>5000 Author-ity clusters searched out of 5000\n",
      "<br><b>Beginning model application for NCT01346709</b><br>You may bookmark this page and return later.<br><b>Found 1 linked articles from PubMed, 1 result references from ClinicalTrials.gov, 1 unique PMIDs</b><br>Applying preselection, this will take about 60 seconds.<BR><b>5000 articles and linked articles have complete data on this server.</b><BR><b>Processing article titles and authors.</b><br><b>Retrieving related article rankings from PubMed.gov</b><BR><b>Retrieving <a href=\"http://arrowsmith.psych.uic.edu/arrowsmith_uic/author2.html\" target=\"_blank\">Author-ity</a> clusters.</b><br><b>Processing article chemicals and accession numbers.</b><br>\n",
      "<br><b>Beginning model application for NCT01346709</b><br>You may bookmark this page and return later.<br><b>Found 1 linked articles from PubMed, 1 result references from ClinicalTrials.gov, 1 unique PMIDs</b><br>Applying preselection, this will take about 60 seconds.<BR><b>5000 articles and linked articles have complete data on this server.</b><BR><b>Processing article titles and authors.</b><br><b>Retrieving related article rankings from PubMed.gov</b><BR><b>Retrieving <a href=\"http://arrowsmith.psych.uic.edu/arrowsmith_uic/author2.html\" target=\"_blank\">Author-ity</a> clusters.</b><br><b>Processing article chemicals and accession numbers.</b><br><b>Processing article titles and abstracts.</b><br>\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'NCT01346709'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 1438\u001b[0m\n\u001b[0;32m   1436\u001b[0m \u001b[39mfor\u001b[39;00m mainindex,mainrow \u001b[39min\u001b[39;00m nset\u001b[39m.\u001b[39miterrows():\n\u001b[0;32m   1437\u001b[0m   ttu \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m-> 1438\u001b[0m   trialdict \u001b[39m=\u001b[39m  alltrialdict[mainrow[\u001b[39m'\u001b[39;49m\u001b[39mNCT_num\u001b[39;49m\u001b[39m'\u001b[39;49m]]\n\u001b[0;32m   1439\u001b[0m   timedict[\u001b[39m'\u001b[39m\u001b[39mctvalues\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(((time\u001b[39m.\u001b[39mtime()\u001b[39m-\u001b[39mttu)\u001b[39m*\u001b[39m\u001b[39m1000\u001b[39m))\n\u001b[0;32m   1440\u001b[0m   newrow \u001b[39m=\u001b[39m modelform\u001b[39m.\u001b[39mcopy()\n",
      "\u001b[1;31mKeyError\u001b[0m: 'NCT01346709'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "#Created on Thu Aug  1 10:43:18 2019\n",
    "#@author: Arthur Holt\n",
    "#\n",
    "#The program performs an example model application for the the model described in \n",
    "#Smalheiser NR, Holt AW (2021). A Web-based Tool for Automatically linking Clinical Trials to their Publications.\n",
    "#https://www.medrxiv.org/content/10.1101/2021.06.24.21259481v1\n",
    "\n",
    "\n",
    "\n",
    "#needed python modules\n",
    "#import mysql.connector\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import subprocess\n",
    "from io import StringIO\n",
    "from nameparser import HumanName\n",
    "import sys\n",
    "import datetime\n",
    "import itertools\n",
    "from scipy import stats\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pickle\n",
    "import urllib\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "amtesting = 'Y'\n",
    "\n",
    "#########################################\n",
    "# locale information\n",
    "#########################################\n",
    "#These variables are used by the web app for storing files in permission-specific areas\n",
    "#for this example, they all point to the location from which this program is run\n",
    "appdir = os.getcwd()\n",
    "cachedir = appdir\n",
    "cachetest = cachedir\n",
    "baseDir = appdir\n",
    "os.chdir(baseDir)\n",
    "\n",
    "if amtesting == 'Y':\n",
    "  cachetest = appdir\n",
    "\n",
    "\n",
    "\n",
    "#time variables that are used by the webserver to show approximate completion %\n",
    "basestart = datetime.datetime.now()\n",
    "esttime = 60*10\n",
    "\n",
    "\n",
    "#This function updates the live web page as scoring is run.  Here, it just prints.\n",
    "def outstat(mtxt,j):\n",
    "  if amtesting == 'Y':\n",
    "    print(mtxt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########################################\n",
    "# job information\n",
    "#########################################\n",
    "job='0XF28DD'\n",
    "\n",
    "\n",
    "\n",
    "#this variable is used throughout this script for the NCT number\n",
    "# trial = 'NCT03745053'\n",
    "trial = 'NCT01346709'\n",
    "\n",
    "\n",
    "statmsg ='<br><b>Beginning model application for ' + trial +  '</b><br>You may bookmark this page and return later.<br>'\n",
    "\n",
    "outstat(statmsg,job)\n",
    "\n",
    "trialquery = 'P'\n",
    "\n",
    "#########################################\n",
    "# model parameters\n",
    "#########################################\n",
    "\n",
    "\n",
    "\n",
    "#list of features, pubmed fields, and methods to compute\n",
    "flist = \"\"\"F03,pmid,aggmax\n",
    "F01,authors,authtree\n",
    "F16,titleabs,propoccurance\n",
    "F13,titleabs,tsim\n",
    "F24,mesh,meshsim\n",
    "F31,authors,authtree\n",
    "F30,abstract,occurance\n",
    "FTA,ta_allcaps,mismatch\n",
    "F25,mesh,meshsim\n",
    "F33,grants,occurance\n",
    "F29,grants,occurance\n",
    "F15,titleabs,propoccurance\n",
    "F02,titleabs,allcaps\n",
    "F13,chemicals,reverseoccur\n",
    "F27,abstract,occurance\n",
    "F30,cn,occurance\n",
    "F09,titleabs,occurance\n",
    "F10,titleabs,occurance\n",
    "F36,authors,authtree\"\"\"\n",
    "\n",
    "\n",
    "#regression parameters\n",
    "MODo={}\n",
    "MODo['F03_pmid_aggmax'] = float(4.204533)\n",
    "MODo['authcomp'] = float(0.035287)\n",
    "MODo['F16_titleabs_propoccurance'] = float(1.295308)\n",
    "MODo['F13_titleabs_tsimw_scr'] = float(0.000234)\n",
    "MODo['F24_mesh_meshsim'] = float(1.526459)\n",
    "MODo['startdate_lift'] = float(1.017043)\n",
    "MODo['F30_abstract_occurance'] = float(1.401932)\n",
    "MODo['F25_mesh_meshsim'] = float(0.766648)\n",
    "MODo['F02_titleabs_allcaps'] = float(0.796624)\n",
    "MODo['F33_grants_occurance'] = float(2.137037)\n",
    "MODo['F29_grants_occurance'] = float(1.10225)\n",
    "MODo['FTA_ta_allcaps_mismatch'] = float(-1.270504)\n",
    "MODo['F15_titleabs_propoccurance'] = float(0.496415)\n",
    "MODo['F27_abstract_occurance'] = float(2.082105)\n",
    "MODo['F13_chemicals_reverseoccur'] = float(2.572171)\n",
    "MODo['F10_titleabs_occurance'] = float(0.39156)\n",
    "MODo['F30_cn_occurance'] = float(2.935671)\n",
    "MODo['F09_titleabs_occurance'] = float(1.404767)\n",
    "\n",
    "MODo_intercept = float(-2.34807)\n",
    "\n",
    "\n",
    "\n",
    "#regression parameters for model 11, only difference is aggmax and authcomp are combined into one feature\n",
    "MOD={}\n",
    "MOD['agginvint'] = float(7.064431)\n",
    "MOD['F16_titleabs_propoccurance'] = float(1.321135)\n",
    "MOD['F13_titleabs_tsimw_scr'] = float(0.000237)\n",
    "MOD['F24_mesh_meshsim'] = float(1.54084)\n",
    "MOD['startdate_lift'] = float(1.03154)\n",
    "MOD['F30_abstract_occurance'] = float(1.371825)\n",
    "MOD['F25_mesh_meshsim'] = float(0.794547)\n",
    "MOD['F02_titleabs_allcaps'] = float(0.805654)\n",
    "MOD['F29_grants_occurance'] = float(1.19076)\n",
    "MOD['F33_grants_occurance'] = float(2.002089)\n",
    "MOD['FTA_ta_allcaps_mismatch'] = float(-1.290924)\n",
    "MOD['F15_titleabs_propoccurance'] = float(0.50953)\n",
    "MOD['F27_abstract_occurance'] = float(2.118923)\n",
    "MOD['F13_chemicals_reverseoccur'] = float(2.641784)\n",
    "MOD['F10_titleabs_occurance'] = float(0.412927)\n",
    "MOD['F30_cn_occurance'] = float(2.980237)\n",
    "MOD['F09_titleabs_occurance'] = float(1.448419)\n",
    "\n",
    "MOD_intercept = float(-3.516664)\n",
    "\n",
    "\n",
    "\n",
    "#mean substitution\n",
    "MODsub = {}\n",
    "MODsub['F03_pmid_aggmax'] = float(0.14455864)\n",
    "MODsub['agginvint'] = float(0.244589999579225)\n",
    "MODsub['authcomp'] = float(0)\n",
    "MODsub['F16_titleabs_propoccurance'] = float(0)\n",
    "MODsub['F13_titleabs_tsimw_scr'] = float(2608.84660876887)\n",
    "MODsub['F24_mesh_meshsim'] = float(0.285332373297017)\n",
    "MODsub['startdate_lift'] = float(1.07582781804635)\n",
    "MODsub['F30_abstract_occurance'] = float(0)\n",
    "MODsub['F25_mesh_meshsim'] = float(0.506728134364736)\n",
    "MODsub['F02_titleabs_allcaps'] = float(0.158897310349453)\n",
    "MODsub['F33_grants_occurance'] = float(0)\n",
    "MODsub['F29_grants_occurance'] = float(0)\n",
    "MODsub['FTA_ta_allcaps_mismatch'] = float(0)\n",
    "MODsub['F15_titleabs_propoccurance'] = float(0)\n",
    "MODsub['F27_abstract_occurance'] = float(0)\n",
    "MODsub['F13_chemicals_reverseoccur'] = float(0.123653040492693)\n",
    "MODsub['F10_titleabs_occurance'] = float(0)\n",
    "MODsub['F30_cn_occurance'] = float(0)\n",
    "MODsub['F09_titleabs_occurance'] = float(0)\n",
    "\n",
    "\n",
    "MODnice = {}\n",
    "MODnice['F03_pmid_aggmax']='Aggregator'\n",
    "MODnice['agginvint']='Investigator/Aggregator Max'\n",
    "MODnice['authcomp']='Investigator->authors'\n",
    "MODnice['F16_titleabs_propoccurance']='Interventions->title/abs'\n",
    "MODnice['F13_titleabs_tsimw_scr']='Text similarity'\n",
    "MODnice['F24_mesh_meshsim']='Interventions MeSH->article MeSH'\n",
    "MODnice['startdate_lift']='Pub Date'\n",
    "MODnice['F30_abstract_occurance']='Acronym->abstract'\n",
    "MODnice['F25_mesh_meshsim']='Condition MeSH->article MeSH'\n",
    "MODnice['F02_titleabs_allcaps']='Title->title/abs All Caps'\n",
    "MODnice['F33_grants_occurance']='Secondary ID->Grants'\n",
    "MODnice['F29_grants_occurance']='Country->Grants'\n",
    "MODnice['FTA_ta_allcaps_mismatch']='Trial->Article All Caps Mismatch'\n",
    "MODnice['F15_titleabs_propoccurance']='Conditions->title/abs'\n",
    "MODnice['F27_abstract_occurance']='Study ID->Abstract'\n",
    "MODnice['F13_chemicals_reverseoccur']='Summary->chemicals'\n",
    "MODnice['F10_titleabs_occurance']='Allocation->title/abs'\n",
    "MODnice['F30_cn_occurance']='Acronym->CN'\n",
    "MODnice['F09_titleabs_occurance']='Study Type->title/abs'\n",
    "\n",
    "\n",
    "#this is used to reduce the size of the \"Noteworthy Features\"\n",
    "MODmap = {}\n",
    "MODmap['F03_pmid_aggmax']='A'\n",
    "MODmap['agginvint']='B'\n",
    "MODmap['authcomp']='C'\n",
    "MODmap['F16_titleabs_propoccurance']='D'\n",
    "MODmap['F13_titleabs_tsimw_scr']='E'\n",
    "MODmap['F24_mesh_meshsim']='F'\n",
    "MODmap['startdate_lift']='G'\n",
    "MODmap['F30_abstract_occurance']='H'\n",
    "MODmap['F25_mesh_meshsim']='I'\n",
    "MODmap['F02_titleabs_allcaps']='J'\n",
    "MODmap['F33_grants_occurance']='K'\n",
    "MODmap['F29_grants_occurance']='L'\n",
    "MODmap['FTA_ta_allcaps_mismatch']='M'\n",
    "MODmap['F15_titleabs_propoccurance']='N'\n",
    "MODmap['F27_abstract_occurance']='O'\n",
    "MODmap['F13_chemicals_reverseoccur']='P'\n",
    "MODmap['F10_titleabs_occurance']='Q'\n",
    "MODmap['F30_cn_occurance']='R'\n",
    "MODmap['F09_titleabs_occurance']='S'\n",
    "\n",
    "#reverse lookup\n",
    "\n",
    "MODmapR = {}\n",
    "\n",
    "for k in MODmap.keys():\n",
    "  MODmapR[MODmap[k]] = k\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#trial start date difference lift\n",
    "startcast = {}\n",
    "startcast[-1]=0\n",
    "startcast[0]=0\n",
    "startcast[1]=0\n",
    "startcast[2]=0\n",
    "startcast[3]=0\n",
    "startcast[4]=0\n",
    "startcast[5]=0.26100307062436\n",
    "startcast[6]=0.211873080859774\n",
    "startcast[7]=0.15532241555783\n",
    "startcast[8]=0.112205731832139\n",
    "startcast[9]=0.0766376663254861\n",
    "startcast[10]=0.0538638689866939\n",
    "startcast[11]=0.0317297850562947\n",
    "startcast[12]=0.029042988741044\n",
    "startcast[13]=0.0180399181166837\n",
    "startcast[14]=0.0152251791197543\n",
    "startcast[15]=0.0097236438075742\n",
    "startcast[16]=0.00895598771750256\n",
    "startcast[17]=0.00588536335721596\n",
    "startcast[18]=0.00537359263050153\n",
    "startcast[19]=0.00230296827021494\n",
    "startcast[20]=0.00281473899692937\n",
    "startcast[21]=0\n",
    "\n",
    "startlift = {}\n",
    "startlift[0]=0.0791550925925926\n",
    "startlift[1]=0.235266203703704\n",
    "startlift[2]=0.798333333333333\n",
    "startlift[3]=1.25251157407407\n",
    "startlift[4]=1.34796296296296\n",
    "startlift[5]=1.28313657407407\n",
    "startlift[6]=1.29091435185185\n",
    "startlift[7]=1.25299768518519\n",
    "startlift[8]=1.10243055555556\n",
    "startlift[9]=1.05590277777778\n",
    "startlift[10]=0.996041666666667\n",
    "startlift[11]=0.944351851851852\n",
    "startlift[12]=1.38649305555556\n",
    "startlift[13]=1.21094907407407\n",
    "startlift[14]=1.57599537037037\n",
    "startlift[15]=1.43228009259259\n",
    "startlift[16]=1.36520833333333\n",
    "\n",
    "#dictionary to map author counts to Aggregator values\n",
    "aggdict = {}\n",
    "aggdict[0]=0.236849605005533\n",
    "aggdict[1]=0.799776153180279\n",
    "aggdict[2]=0.858079976654628\n",
    "aggdict[3]=0.879815195028985\n",
    "aggdict[4]=0.922516947999999\n",
    "aggdict[5]=0.924174518944444\n",
    "aggdict[6]=0.947440810421052\n",
    "aggdict[7]=0.934076053928571\n",
    "aggdict[8]=0.9523414651875\n",
    "aggdict[9]=0.990714104235294\n",
    "\n",
    "\n",
    "\n",
    "#xml paths for trial data from clinicaltrials.gov\n",
    "ctxpath = {}\n",
    "ctxpath['F01'] = ['//clinical_study/overall_official/last_name']\n",
    "ctxpath['F02'] = ['//clinical_study/brief_title']\n",
    "ctxpath['F09'] = ['//clinical_study/study_type']\n",
    "ctxpath['F10'] = ['//clinical_study/study_design_info/allocation']\n",
    "ctxpath['F13'] = ['//clinical_study/brief_summary/textblock']\n",
    "ctxpath['F15'] = ['//clinical_study/condition']\n",
    "ctxpath['F16'] = ['//clinical_study/intervention/intervention_name']\n",
    "ctxpath['F24'] = ['//clinical_study/intervention_browse/mesh_term']\n",
    "ctxpath['F25'] = ['//clinical_study/condition_browse/mesh_term']\n",
    "ctxpath['F27'] = ['//clinical_study/id_info/org_study_id']\n",
    "ctxpath['F29'] = ['//clinical_study/location_countries/country']\n",
    "ctxpath['F30'] = ['//clinical_study/acronym']\n",
    "ctxpath['F31'] = ['//clinical_study/responsible_party/investigator_full_name']\n",
    "ctxpath['F33'] = ['//clinical_study/id_info/secondary_id']\n",
    "ctxpath['F36'] = ['//clinical_study/clinical_results/point_of_contact/name_or_title']\n",
    "\n",
    "\n",
    "#dictionary to convert model probabilities to odds ratios for rescaling\n",
    "oddsdict = {}\n",
    "oddsdict[0]=.000000980894\n",
    "oddsdict[0.01]=.000010909012\n",
    "oddsdict[0.02]=.000030083876\n",
    "oddsdict[0.03]=.000052726013\n",
    "oddsdict[0.04]=.000107714488\n",
    "oddsdict[0.05]=.000130691128\n",
    "oddsdict[0.06]=.000168898908\n",
    "oddsdict[0.07]=.000271835061\n",
    "oddsdict[0.08]=.000280245585\n",
    "oddsdict[0.09]=.000293219365\n",
    "oddsdict[0.1]=.000453680491\n",
    "oddsdict[0.11]=.000520910748\n",
    "oddsdict[0.12]=.000365614418\n",
    "oddsdict[0.13]=.000462469359\n",
    "oddsdict[0.14]=.000553654586\n",
    "oddsdict[0.15]=.000766747333\n",
    "oddsdict[0.16]=.00067358043\n",
    "oddsdict[0.17]=.000798057084\n",
    "oddsdict[0.18]=.000945916233\n",
    "oddsdict[0.19]=.000786674193\n",
    "oddsdict[0.2]=.000784539495\n",
    "oddsdict[0.21]=.000916537767\n",
    "oddsdict[0.22]=.001059894371\n",
    "oddsdict[0.23]=.0012932813\n",
    "oddsdict[0.24]=.001167936043\n",
    "oddsdict[0.25]=.00116234332\n",
    "oddsdict[0.26]=.00168337759\n",
    "oddsdict[0.27]=.00150080305\n",
    "oddsdict[0.28]=.001496369729\n",
    "oddsdict[0.29]=.001827211394\n",
    "oddsdict[0.3]=.001201957066\n",
    "oddsdict[0.31]=.001681508993\n",
    "oddsdict[0.32]=.001884282263\n",
    "oddsdict[0.33]=.001741317283\n",
    "oddsdict[0.34]=.002063840322\n",
    "oddsdict[0.35]=.002467667587\n",
    "oddsdict[0.36]=.002020441346\n",
    "oddsdict[0.37]=.002540659097\n",
    "oddsdict[0.38]=.002877843944\n",
    "oddsdict[0.39]=.002602175152\n",
    "oddsdict[0.4]=.00208715538\n",
    "oddsdict[0.41]=.002468806121\n",
    "oddsdict[0.42]=.003221488068\n",
    "oddsdict[0.43]=.002645933331\n",
    "oddsdict[0.44]=.00391710469\n",
    "oddsdict[0.45]=.003393614126\n",
    "oddsdict[0.46]=.003268307906\n",
    "oddsdict[0.47]=.003126243622\n",
    "oddsdict[0.48]=.003161472564\n",
    "oddsdict[0.49]=.004365004998\n",
    "oddsdict[0.5]=.004391538247\n",
    "oddsdict[0.51]=.004745231914\n",
    "oddsdict[0.52]=.003621207074\n",
    "oddsdict[0.53]=.004318455691\n",
    "oddsdict[0.54]=.00412870284\n",
    "oddsdict[0.55]=.005488120739\n",
    "oddsdict[0.56]=.006002718567\n",
    "oddsdict[0.57]=.005267462797\n",
    "oddsdict[0.58]=.005278684021\n",
    "oddsdict[0.59]=.006452648187\n",
    "oddsdict[0.6]=.005768121771\n",
    "oddsdict[0.61]=.007399729768\n",
    "oddsdict[0.62]=.008908729576\n",
    "oddsdict[0.63]=.007832669926\n",
    "oddsdict[0.64]=.006445595567\n",
    "oddsdict[0.65]=.010158397427\n",
    "oddsdict[0.66]=.010557800364\n",
    "oddsdict[0.67]=.009845436328\n",
    "oddsdict[0.68]=.011673799726\n",
    "oddsdict[0.69]=.012421854964\n",
    "oddsdict[0.7]=.012037083613\n",
    "oddsdict[0.71]=.011029480861\n",
    "oddsdict[0.72]=.014187264289\n",
    "oddsdict[0.73]=.013930215183\n",
    "oddsdict[0.74]=.016788921613\n",
    "oddsdict[0.75]=.018567606025\n",
    "oddsdict[0.76]=.018835279687\n",
    "oddsdict[0.77]=.021220179551\n",
    "oddsdict[0.78]=.024055845325\n",
    "oddsdict[0.79]=.02322490494\n",
    "oddsdict[0.8]=.027167187971\n",
    "oddsdict[0.81]=.035791250199\n",
    "oddsdict[0.82]=.038623210933\n",
    "oddsdict[0.83]=.03924893171\n",
    "oddsdict[0.84]=.043537351896\n",
    "oddsdict[0.85]=.058500425012\n",
    "oddsdict[0.86]=.063474118298\n",
    "oddsdict[0.87]=.07950136523\n",
    "oddsdict[0.88]=.086653180387\n",
    "oddsdict[0.89]=.108780978242\n",
    "oddsdict[0.9]=.140013149188\n",
    "oddsdict[0.91]=.180571560211\n",
    "oddsdict[0.92]=.256457230448\n",
    "oddsdict[0.93]=.353362562033\n",
    "oddsdict[0.94]=.480092663109\n",
    "oddsdict[0.95]=.687539392344\n",
    "oddsdict[0.96]=1.202707010962\n",
    "oddsdict[0.97]=2.233825247408\n",
    "oddsdict[0.98]=4.747515196605\n",
    "oddsdict[0.99]=22.7512738835\n",
    "oddsdict[1]=81.\n",
    "\n",
    "\n",
    "def adjustOdds(inProb):\n",
    "  if inProb <= 0:\n",
    "    return 0\n",
    "  if inProb >= 1:\n",
    "    return 1\n",
    "  problow = np.floor(inProb*100)/100\n",
    "  probhigh = np.ceil(inProb*100)/100\n",
    "  if probhigh > problow:\n",
    "    probdist = (inProb - problow) / (probhigh - problow)\n",
    "  else:\n",
    "    probdist = 0\n",
    "  print(problow,probhigh,probdist)\n",
    "  oddslow = oddsdict[problow]\n",
    "  oddshigh = oddsdict[probhigh]\n",
    "  oddsadjust = oddslow + (probdist*(oddshigh-oddslow))\n",
    "  return 1/(1+(1/oddsadjust))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#convert feature list to a pandas dataframe\n",
    "flistdf = pd.DataFrame(columns=['featnum','pubmed','method'], \n",
    "  data=[row.split(',') for row in flist.splitlines()])\n",
    "flistdf['pubmedlist'] = flistdf.apply(lambda row: row['pubmed'].split('|'), axis=1)\n",
    "flistdf['methodlist'] = flistdf.apply(lambda row: row['method'].split('|'), axis=1)\n",
    "\n",
    "#create base dataframe that will hold the computed fields\n",
    "clist = ['sample_id','posneg', 'testcontrol','NCT_num','PMID']\n",
    "for index, row in flistdf.iterrows():\n",
    "  for p in row['pubmedlist']:\n",
    "    for m in row['methodlist']:\n",
    "      if m in ['tsim','rksimax']:\n",
    "        clist.extend([row['featnum']+'_'+p+'_'+m + 'unw_tms'])\n",
    "        clist.extend([row['featnum']+'_'+p+'_'+m + 'unw_scr'])\n",
    "        clist.extend([row['featnum']+'_'+p+'_'+m + 'w_tms'])\n",
    "        clist.extend([row['featnum']+'_'+p+'_'+m + 'w_scr'])\n",
    "      else:\n",
    "        clist.extend([row['featnum']+'_'+p+'_'+m])\n",
    "\n",
    "clist.append('startdate_lift')\n",
    "clist.append('start_diff_lt5')\n",
    "clist.append('start_diff_cast')\n",
    "clist.append('authcomp')\n",
    "clist.append('agginvint')\n",
    "clist.append('prob')\n",
    "modelform = pd.DataFrame(columns=clist)\n",
    "modeldata = modelform.copy()\n",
    "\n",
    "\n",
    "\n",
    "############################################\n",
    "#import functions here\n",
    "############################################\n",
    "\n",
    "#for this example, all files are located in the current folder\n",
    "lfiles = ''\n",
    "\n",
    "\n",
    "#this was not implemented as a package\n",
    "exec(open('TrialLink_functions_Example.py').read())\n",
    "\n",
    "#regular expression to pick out capitalized words\n",
    "caps = re.compile(r'\\b([A-Z]{3,})\\b') \n",
    "#regular expression to pick out nct number\n",
    "nlink = re.compile('\\[.*?\\]') \n",
    "#regular expression to puck out capitals\n",
    "fletter = re.compile(r'\\b[A-Z]')\n",
    "\n",
    "\n",
    "#stop list\n",
    "#general stop words\n",
    "with open (lfiles + 'stoplist.pkl','rb') as f:\n",
    "  stoplist = pickle.load(f)\n",
    "\n",
    "\n",
    "#nickname list\n",
    "with open (lfiles + 'nicknames.pkl','rb') as f:\n",
    "  nickdict = pickle.load(f)\n",
    "\n",
    "\n",
    "#stop list for MeSH\n",
    "with open (lfiles + 'stoplistmesh.pkl','rb') as f:\n",
    "  stoplistmesh = pickle.load(f)\n",
    "\n",
    "MeSHstop = stoplistmesh\n",
    "\n",
    "#country list, converted to a dictionary for speed\n",
    "#countriesdf = pd.read_csv(lfiles + 'country_list2.csv')\n",
    "#countries = dict([(observed,official) for observed,official in zip(countriesdf.observed,countriesdf.official)])\n",
    "#with open (lfiles + 'countries.pkl','wb') as f:\n",
    "#  pickle.dump(countries,f)\n",
    "with open (lfiles + 'countries.pkl','rb') as f:\n",
    "  countries = pickle.load(f)\n",
    "\n",
    "#ADAM abbreviation list\n",
    "with open (lfiles + 'adam.pkl','rb') as f:\n",
    "  adam = pickle.load(f)\n",
    "\n",
    "\n",
    "#backslash is a problematic delimiter for parts of the name\n",
    "escape_dict={'\\a':r'\\a',\n",
    "             '\\b':r'\\b',\n",
    "             '\\c':r'\\c',\n",
    "             '\\f':r'\\f',\n",
    "             '\\n':r'\\n',\n",
    "             '\\r':r'\\r',\n",
    "             '\\t':r'\\t',\n",
    "             '\\v':r'\\v',\n",
    "             '\\'':r'\\'',\n",
    "             '\\\"':r'\\\"'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#function to break up lists to sizes that sql code can handle in a where clause\n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "\n",
    "\n",
    "#new function added for the 'mis-matched all caps feature'\n",
    "\n",
    "def comp_mismatch(csett,ptxt):\n",
    "  #print(csett,ptxt)\n",
    "  cval = 0\n",
    "  if len(csett)==1 and len(ptxt)>0:\n",
    "    t = list(csett)[0]\n",
    "    if t not in adam2:\n",
    "      cval = 1\n",
    "      mcand = ptxt\n",
    "      mcandin = mcand\n",
    "      for w in mcand:\n",
    "        if levenshtein(w,t) < 3:\n",
    "          cval=0\n",
    "  if len(ptxt)==1 and len(csett)>0:\n",
    "    t = list(ptxt)[0]\n",
    "    if t not in adam2:\n",
    "      cval = 1\n",
    "      mcand = csett\n",
    "      mcandin = mcand\n",
    "      for w in mcand:\n",
    "        if levenshtein(w,t) < 3:\n",
    "          cval=0\n",
    "  return cval\n",
    "\n",
    "\n",
    "#new, faster ADAM list\n",
    "adam2 = set()\n",
    "for a in adam:\n",
    "  for b in a.split('|'):\n",
    "    if ':' in b:\n",
    "      adam2.add(b[:b.find(':')].upper())\n",
    "    else:\n",
    "      adam2.add(b.upper())\n",
    "\n",
    "#function to remove stop words and ADAM abbrevations\n",
    "def suppressWords(inWord):\n",
    "  result = inWord.upper()\n",
    "  if result in stoplist:\n",
    "    result = 'remove'\n",
    "  if result in adam2:\n",
    "    result = 'remove'\n",
    "  return result\n",
    "\n",
    "\n",
    "\n",
    "############################################\n",
    "#find already-linked articles \n",
    "############################################\n",
    "passure  = '0'\n",
    "searchres = []\n",
    "searchlinks = []\n",
    "\n",
    "#nct links\n",
    "selectSQL = 'select c.pmid from PUBMED2015.ctmap c WHERE c.ctid_cleaned = \"{0}\"    ;'\n",
    "#dbcursor.execute(selectSQL.format(trial) )    \n",
    "#rks = dbcursor.fetchall()\n",
    "with open('dataset_01.pkl', 'rb') as f:\n",
    "  rks = pickle.load(f)\n",
    "\n",
    "for r in rks:\n",
    "  searchlinks.append(r[0])\n",
    "\n",
    "\n",
    "\n",
    "#results reference\n",
    "selectSQL = '''select a.value from clinicaltrials.clinicaltrials a JOIN clinicaltrials.xmlpaths b on a.xmlpathid = b.xmlpathid \n",
    "WHERE b.xmlpath = '//clinical_study/results_reference/PMID' and a.NCT_num = \"{0}\"    ;'''\n",
    "#dbcursor.execute(selectSQL.format(trial) )    \n",
    "#rks = dbcursor.fetchall()\n",
    "with open('dataset_02.pkl', 'rb') as f:\n",
    "  rks = pickle.load(f)\n",
    "\n",
    "for r in rks:\n",
    "  searchres.append(int(r[0]))\n",
    "\n",
    "\n",
    "passureT = set(searchlinks) | set(searchres)\n",
    "passure = ','.join([str(v) for v in list(passureT)])\n",
    "\n",
    "statmsg += '<b>Found ' + str(len(searchlinks)) + ' linked articles from PubMed, ' + str(len(searchres)) + ' result references from ClinicalTrials.gov, '+ str(len(passureT)) + ' unique PMIDs</b><br>'\n",
    "if trialquery == 'P':\n",
    "  statmsg += 'Applying preselection, this will take about 60 seconds.<BR>'\n",
    "\n",
    "outstat(statmsg,job)\n",
    "\n",
    "\n",
    "\n",
    "#####################################\n",
    "#get prospective pmids for this trial\n",
    "#####################################\n",
    "with open('stoplist.txt', 'r') as file:\n",
    "    stopA = file.read()\n",
    "\n",
    "with open('stoplist1000.txt', 'r') as file:\n",
    "    stopB = file.read()\n",
    "\n",
    "stopadd = '''\n",
    "PATIENTS PURPOSE PLACEBO EVALUATE\n",
    "DETERMINE EFFECTS SUBJECTS ASSESS\n",
    "DISEASES AIM MONTHS WEEKS\n",
    "OBJECTIVE YEARS - INVESTIGATORS\n",
    "WELL PARTICIPANTS RECEIVE TREATING\n",
    "CELLS TOLERABILITY CELL OUTCOMES\n",
    "DAILY DRUGS ADULTS CHEMOTHERAPY\n",
    "CONDUCTED INFECTIONS STUDIES DESIGNED\n",
    "SYMPTOMS DISORDERS GIVEN\n",
    "'''\n",
    "\n",
    "listA = [v.upper() for v in stopA.split()]\n",
    "listB = [v.upper() for v in stopB.split()]\n",
    "listC = stopadd.split()\n",
    "\n",
    "\n",
    "stopAll = set(listA) | set(listB) |  set(listC)\n",
    "stop365 =  set(listA) | set(listC)\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def cleanword(w):\n",
    "  tw = w.upper()\n",
    "  if tw in stopAll:\n",
    "    tw = ' '\n",
    "  if is_number(tw):\n",
    "    tw = ' '\n",
    "  if len(tw)==1:\n",
    "    tw = ' '\n",
    "  return tw\n",
    "\n",
    "def cleanword365(w):\n",
    "  tw = w.upper()\n",
    "  if tw in stop365:\n",
    "    tw = ' '\n",
    "  if is_number(tw):\n",
    "    tw = ' '\n",
    "  if len(tw)==1:\n",
    "    tw = ' '\n",
    "  return tw\n",
    "\n",
    "\n",
    "selectSQL = 'SELECT ctindex, trial_start_d, refdate_min from clinicaltrials.ctlookup WHERE NCT_num=\"' + trial + '\" ;'\n",
    "#dbcursor.execute(selectSQL)     \n",
    "#res = dbcursor.fetchall()\n",
    "\n",
    "with open('dataset_03.pkl', 'rb') as f:\n",
    "  res = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "if len(res) == 1:\n",
    "  trialindex = res[0][0]\n",
    "  trialstart = res[0][1]\n",
    "  refdate = res[0][2]\n",
    "\n",
    "if refdate is not None:\n",
    "  y = str(refdate.year - 2)\n",
    "else:\n",
    "  y = '1987'\n",
    "\n",
    "\n",
    "trial_condpick = ['//clinical_study/condition_browse/mesh_term','//clinical_study/condition']\n",
    "trial_intpick = ['//clinical_study/intervention/intervention_name','//clinical_study/intervention/other_name',\n",
    "'//clinical_study/intervention/arm_group_label','//clinical_study/intervention_browse/mesh_term','//clinical_study/intervention/description']\n",
    "mesh_synpick = 'select syn from clinicaltrials.ctgov_MeSHtrans where mesh_term like \"{0}\";'\n",
    "\n",
    "\n",
    "psql = '''select a.value from clinicaltrials.clinicaltrials a \n",
    " where a.NCT_num = \"{0}\" and a.xmlpathid in \n",
    " (select b.xmlpathid from clinicaltrials.xmlpaths b where b.xmlpath in ({1}) );  '''\n",
    "\n",
    "matchcount = '''select PMID \n",
    "  from clinicaltrials.pub_words  \n",
    "  where year(pubdate) > {0} and {1}   '''\n",
    "\n",
    "matchcountD = '''select PMID, {2} as wcount, {3} as ccount, {4} as icount  \n",
    "  from clinicaltrials.pub_words  \n",
    "  where year(pubdate) > {0} and {1}   '''\n",
    "\n",
    "\n",
    "#trial title and descriptions\n",
    "#dbcursor.execute(psql.format(trial,'\"//clinical_study/brief_title\"'))\n",
    "#rks = dbcursor.fetchall()\n",
    "\n",
    "with open('dataset_04.pkl', 'rb') as f:\n",
    "  rks = pickle.load(f)\n",
    "\n",
    "if len(rks) > 0:\n",
    "  ttitle = rks[0][0]\n",
    "\n",
    "\n",
    "if len(ttitle) < 2:\n",
    "  #dbcursor.execute(psql.format(trial,'\"//clinical_study/official_title\"'))\n",
    "  #rks = dbcursor.fetchall()\n",
    "  if len(rks) > 0:\n",
    "    ttitle = rks[0][0]\n",
    "\n",
    "\n",
    "\n",
    "tdesc = ''\n",
    "#dbcursor.execute(psql.format(trial,'\"//clinical_study/brief_summary/textblock\",\"//clinical_study/brief_summary/textblock\",\"//clinical_study/detailed_description/textblock\"'))\n",
    "#rks = dbcursor.fetchall()\n",
    "with open('dataset_05.pkl', 'rb') as f:\n",
    "  rks = pickle.load(f)\n",
    "\n",
    "\n",
    "if len(rks) > 0:\n",
    "  tdesc = ' ' + rks[0][0]\n",
    "\n",
    "tstring = re.sub(r'[()*=±%;:|.,\"]', ' ', ttitle + ' ' + tdesc)\n",
    "tcomb = ' '.join([cleanword(v.upper()) for v in tstring.split()]) \n",
    "tcomb = ' '.join(tcomb.split())\n",
    "#nset.at[index,'twords'] = tcomb\n",
    "\n",
    "\n",
    "######################### #conditions\n",
    "hascond = 0\n",
    "condlist = []\n",
    "#dbcursor.execute(psql.format(trial,','.join(['\"'+v+'\"' for v in trial_condpick])))\n",
    "#rks = dbcursor.fetchall()\n",
    "with open('dataset_06.pkl', 'rb') as f:\n",
    "  rks = pickle.load(f)\n",
    "\n",
    "\n",
    "#if len(rks) > 0:\n",
    "#  for r in rks:\n",
    "#    condlist.append(cleanword365(r[0].upper()))\n",
    "#    dbcursor.execute(mesh_synpick.format(r[0].upper()))\n",
    "#    rks2 = dbcursor.fetchall()\n",
    "#    for r2 in rks2:\n",
    "#      hascond=1\n",
    "#      condlist.append(r2[0].upper())\n",
    "\n",
    "with open('dataset_07.pkl', 'rb') as f:\n",
    "  condlist = pickle.load(f)\n",
    "\n",
    "\n",
    "condlist = set(condlist) - set([' '])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################### interventions\n",
    "intlist = []\n",
    "#dbcursor.execute(psql.format(trial,','.join(['\"'+v+'\"' for v in trial_intpick])))\n",
    "#rks = dbcursor.fetchall()\n",
    "\n",
    "with open('dataset_08.pkl', 'rb') as f:\n",
    "  rks = pickle.load(f)\n",
    "\n",
    "if len(rks) > 0:\n",
    "  for r in rks:\n",
    "    rtemp = re.sub(r'[()*=±%;:|.,\"]', ' ',r[0])\n",
    "    intlist.extend([v.upper() for v in rtemp.split()])\n",
    "\n",
    "intset = set([cleanword365(v) for v in ' '.join(intlist).split()]) - set([' '])\n",
    "inttemp = ' '.join(list(intset))\n",
    "\n",
    "\n",
    "#build where clauses\n",
    "\n",
    "c1where = '+'.join([' (case when match(searchwords) against(\"{0}\") > 0 then 1 else 0 end) '.format(w) for w in list(tcomb.split())])\n",
    "\n",
    "if hascond==1:\n",
    "  c2where = '+'.join([' (case when match(searchwords) against(\\'\"{0}\"\\') > 0 then 1 else 0 end) '.format(w) for w in list(condlist)]) \n",
    "  newcond = condlist\n",
    "else:\n",
    "    condtemp = ' '.join(list(condlist))\n",
    "    newcond = set([cleanword365(w) for w in condtemp.split()])\n",
    "    newcond.discard(' ')\n",
    "    c2where = '+'.join([' (case when match(searchwords) against(\\'\"{0}\"\\') > 0 then 1 else 0 end) '.format(w) for w in list(newcond)]) \n",
    "\n",
    "if len(inttemp) > 2:\n",
    "  c3where = '+'.join([' (case when match(searchwords) against(\"{0}\") > 0 then 1 else 0 end) '.format(w) for w in inttemp.split()]) \n",
    "else:\n",
    "  c3where = '+'.join([' (case when match(searchwords) against(\"X\") > 0 then 1 else 1 end) ']) \n",
    "\n",
    "\n",
    "\n",
    "#these are trial word counts to be used in the preselect query\n",
    "wcount = max(1,len(tcomb.split()))\n",
    "ccount = max(1,len(newcond))\n",
    "icount = max(1,len(intset))\n",
    "wperc = 0.25 * wcount\n",
    "cperc = 0.25 * ccount\n",
    "iperc = 0.27 * icount\n",
    "wpercs1 = 0.32 * len(tcomb.split())\n",
    "\n",
    "\n",
    "rks_base = []\n",
    "\n",
    "#this is the big preselect query, the else runs the pubmed query instead\n",
    "#if trialquery == 'P':\n",
    "#  #look to see if the candidate set has been precomputed\n",
    "#  mpre = ''' select PMID, wcount,ccount,icount from \n",
    "#  clinicaltrials.trial_cand \n",
    "#  where NCT_num = \"{0}\";'''\n",
    "#  dbcursor.execute(mpre.format(trial))\n",
    "#  rks_base = dbcursor.fetchall()\n",
    "#  #get potential candidates from the base article list\n",
    "#  if len(rks_base) == 0:\n",
    "#    mtest = '''select PMID, {1} as c1, {2} as c2, {3} as c3 from \n",
    "#    (select a.PMID, a.searchwords from clinicaltrials.pub_words a where a.pubdate > {0} and ({1}) > 0 ) b  ;'''\n",
    "#    dbcursor.execute(mtest.format(y,c1where,c2where,c3where))\n",
    "#    for r in dbcursor:\n",
    "#      if (r[2]>0 and r[3]> 0) or r[2]>2 or r[3]>3 or r[1]>wpercs1:\n",
    "#        rks_base.append(r)\n",
    "#  #get potential candidates from the new article list\n",
    "#  mtest = '''select PMID, {1} as c1, {2} as c2, {3} as c3 from \n",
    "#    (select a.PMID, a.searchwords from clinicaltrials.pub_words_new a where a.pubdate > {0} and ({1}) > 0 ) b  ;'''\n",
    "#  dbcursor.execute(mtest.format(y,c1where,c2where,c3where))\n",
    "#  for r in dbcursor:\n",
    "#    if (r[2]>0 and r[3]> 0) or r[2]>2 or r[3]>3 or r[1]>wpercs1:\n",
    "#      rks_base.append(r)\n",
    "#  rdf = pd.DataFrame(rks_base,columns=['PMID','wcount','ccount','icount'])\n",
    "#  if len(rdf) > 5000:\n",
    "#    rdf['pavg'] = rdf.apply(lambda row: ((row['wcount']/wcount)+(row['ccount']/ccount)+(row['icount']/icount))/3,axis=1)\n",
    "#    rdftop = rdf.sort_values(['pavg','PMID'],ascending=[False,False]).copy().head(5000)\n",
    "#    pmgovlist = [str(v) for v in rdftop['PMID']]\n",
    "#  else:\n",
    "#    pmgovlist = [str(v) for v in rdf['PMID'] ]\n",
    "#  rdf = None\n",
    "#  rdf_top = None\n",
    "#  rks_base = None\n",
    "#else:\n",
    "#  pubmedstr = urllib.parse.quote_plus(trialquery)\n",
    "#  urlbase = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&id=anne2001&retmode=JSON&retmax=100000&term='\n",
    "#  r=requests.get(urlbase + pubmedstr )\n",
    "#  xt = json.loads(r.text)\n",
    "#  pmgovlist = xt['esearchresult']['idlist']\n",
    "#  pmgovlist = pmgovlist[:100000]\n",
    "\n",
    "\n",
    "with open('dataset_09.pkl', 'rb') as f:\n",
    "  pmgovlist = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "############################################# prepare article dates\n",
    "\n",
    "#get a true date version of the publication dates\n",
    "selectSQL = \"\"\"select PMID, STR_TO_DATE(CONCAT(CONVERT(b.year,CHAR(4)),'-',\n",
    "    (CASE WHEN b.month = '-' THEN 'jun' \n",
    "          WHEN b.month = '01' THEN 'jan' \n",
    "          WHEN b.month = '02' THEN 'feb' \n",
    "          WHEN b.month = '03' THEN 'mar' \n",
    "          WHEN b.month = '04' THEN 'apr' \n",
    "          WHEN b.month = '05' THEN 'may' \n",
    "          WHEN b.month = '06' THEN 'jun' \n",
    "          WHEN b.month = '07' THEN 'jul' \n",
    "          WHEN b.month = '08' THEN 'aug' \n",
    "          WHEN b.month = '09' THEN 'sep' \n",
    "          WHEN b.month = '1' THEN 'jan' \n",
    "          WHEN b.month = '10' THEN 'oct' \n",
    "          WHEN b.month = '11' THEN 'nov' \n",
    "          WHEN b.month = '12' THEN 'dec' \n",
    "          WHEN b.month = '2' THEN 'feb' \n",
    "          WHEN b.month = '3' THEN 'mar' \n",
    "          WHEN b.month = '4' THEN 'apr' \n",
    "          WHEN b.month = '5' THEN 'may' \n",
    "          WHEN b.month = '6' THEN 'jun' \n",
    "          WHEN b.month = '7' THEN 'jul' \n",
    "          WHEN b.month = '8' THEN 'aug' \n",
    "          WHEN b.month = '9' THEN 'sep' \n",
    "          WHEN b.month = 'autumn' THEN 'oct' \n",
    "          WHEN b.month = 'fall' THEN 'oct' \n",
    "          WHEN b.month = 'october' THEN 'oct' \n",
    "          WHEN b.month = 'spring' THEN 'apr' \n",
    "          WHEN b.month = 'summer' THEN 'jul' \n",
    "          WHEN b.month = 'winter' THEN 'jan' \n",
    "      ELSE b.month END)\n",
    "   ,'-15'), '%Y-%b-%d') as pubdate \n",
    "FROM PUBMED2015.Articles b \n",
    "WHERE b.PMID in ({0}) and b.languages like '%eng%'  ;\n",
    "\"\"\"\n",
    "\n",
    "#ptups = []\n",
    "#for chunk in chunker(pmgovlist,100):\n",
    "#  dbcursor.execute(selectSQL.format(','.join(chunk)))\n",
    "#  ptups.extend(dbcursor.fetchall())\n",
    "\n",
    "#for chunk in chunker(list(passureT),100):\n",
    "#  dbcursor.execute(selectSQL.format(','.join([str(v) for v in chunk])))\n",
    "#  ptups.extend(dbcursor.fetchall())\n",
    "\n",
    "\n",
    "with open('dataset_10.pkl', 'rb') as f:\n",
    "  ptups = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "nset = pd.DataFrame(ptups)\n",
    "nset.columns = (['PMID','pubdate'])\n",
    "nset['NCT_num'] = trial\n",
    "nset['trial_start'] = trialstart\n",
    "nctlist = list(set(nset['NCT_num']))\n",
    "pmidlist = list(set(nset['PMID']))\n",
    "\n",
    "statmsg += '<b>' + str(len(pmidlist)) + ' articles and linked articles have complete data on this server.</b><BR>'\n",
    "outstat(statmsg,job)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############  keeping this because it computes number of article links\n",
    "#full list of nct linked articles and pmids\n",
    "selectSQL = \"\"\"select PMID,1 as ctindex, NCT_num, 1 as Nout, SRC\n",
    "FROM clinicaltrials.cleaned_links \n",
    "WHERE NCT_num in ({0})\n",
    "   ;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "linkdict = {}\n",
    "#dbcursor.execute(selectSQL.format(','.join(['\"' + v + '\"' for v in nctlist])))     \n",
    "#rks = dbcursor.fetchall()\n",
    "\n",
    "with open('dataset_11.pkl', 'rb') as f:\n",
    "  rks = pickle.load(f)\n",
    "\n",
    "\n",
    "if len(rks)>0:\n",
    "  linklist = pd.DataFrame(rks,columns = ['PMID','ctindex','NCT_num','Nout','SRC'])\n",
    "  linklist['PMIDc'] = linklist.apply(lambda row: str(row['PMID']), axis=1)\n",
    "  linklist2 = linklist.loc[linklist['Nout']==1].copy()\n",
    "  linkgroup = linklist2.groupby(['NCT_num'])['PMIDc'].apply(','.join).reset_index()\n",
    "  linkgroup.columns=['NCT_num','links']\n",
    "  for index, row in linkgroup.iterrows():\n",
    "    if row['NCT_num'] not in linkdict.keys():\n",
    "      linkdict[row['NCT_num']] = row['links']\n",
    "  nset = nset.merge(linkgroup,left_on='NCT_num',right_on='NCT_num', how='left')\n",
    "  pmidlist.extend(linklist2['PMID'])\n",
    "\n",
    "pmidlist = list(set(pmidlist))\n",
    "\n",
    "\n",
    "\n",
    "#have at least one article in the list to prevent crashing\n",
    "if len(pmidlist) ==0:\n",
    "  pmidlist = [32367616]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "######################## preload Article Data\n",
    "#############################################\n",
    "alltrialdict = {}\n",
    "trialfeat = set(flistdf['featnum'])\n",
    "trialfeat.discard('FTA')\n",
    "trialfeat.discard('F03')\n",
    "\n",
    "#instantiate all the dictionary elements\n",
    "for n in nctlist:\n",
    "  alltrialdict[n] = {}\n",
    "  for f in trialfeat:\n",
    "    alltrialdict[n][f] = ''\n",
    "\n",
    "\n",
    "xsql = '''select a.NCT_num, group_concat(a.value separator \"|\") from clinicaltrials.clinicaltrials a \n",
    " where a.xmlpathid in \n",
    " (select b.xmlpathid from clinicaltrials.xmlpaths b where b.xmlpath in ({0}) )\n",
    " and a.NCT_Num in ({1}) \n",
    " group by a.NCT_num;  '''\n",
    "\n",
    "\n",
    "#for f in trialfeat:\n",
    "#    for chunk in chunker(nctlist,500):\n",
    "#      selectSQL = xsql.format(','.join(['\"' + v + '\"' for v in ctxpath[f]]) ,','.join(['\"' + v + '\"' for v in chunk])  )\n",
    "#      dbcursor.execute(selectSQL)     \n",
    "#      for resrow in dbcursor:\n",
    "#        alltrialdict[resrow[0]][f] = resrow[1]\n",
    "\n",
    "with open('dataset_12.pkl', 'rb') as f:\n",
    "  alltrialdict = pickle.load(f)\n",
    "\n",
    "\n",
    "for n in alltrialdict.keys():\n",
    "  ttemp = ' '.join([suppressWords(v) for v in caps.findall(alltrialdict[n]['F13']) ] ) + ' ' + alltrialdict[n]['F02'] + ' ' +alltrialdict[n]['F27'] + ' '+alltrialdict[n]['F30'] + ' '\n",
    "  alltrialdict[n]['FTA'] = set(caps.findall(ttemp))\n",
    "  alltrialdict[n]['F03'] = n\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############ make main article dataframe now in order to launch text sim early\n",
    "statmsg += '<b>Processing article titles and authors.</b><br>'\n",
    "outstat(statmsg,job)\n",
    "progress = '{0} article titles and authors processed out of ' + str(len(pmidlist))\n",
    "\n",
    "articletups = []\n",
    "\n",
    "aSQL = '''SELECT a.PMID, a.authors, a.affiliation, a.email, a.title, a.grants, a.mesh, b.abstract from PUBMED2015.Articles a \n",
    "  left outer join PUBMED2015.Abstracts b\n",
    "  ON a.PMID=b.PMID\n",
    "  WHERE a.PMID in ('''\n",
    "\n",
    "#for chunk in chunker(pmidlist,500):\n",
    "#  selectSQL = aSQL + ','.join([str(v) for v in chunk]) + ') ;'\n",
    "#  dbcursor.execute(selectSQL)\n",
    "#  articletups.extend(dbcursor.fetchall())\n",
    "\n",
    "with open('dataset_13.pkl', 'rb') as f:\n",
    "  articletups = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "foundlist = []\n",
    "for t in articletups:\n",
    "  foundlist.append(t[0])\n",
    "\n",
    "for v in pmidlist:\n",
    "  if int(v) not in foundlist:\n",
    "    articletups.append((int(v),'-','-','-','-','-','-','-'))\n",
    "\n",
    "\n",
    "article_rows = pd.DataFrame(articletups,columns = ['PMID','authors','affiliation','email', 'title','grants','mesh','abstract'])\n",
    "articletups = []\n",
    "\n",
    "\n",
    "#These files are processed with the text similarity model described in:\n",
    "#http://arrowsmith.psych.uic.edu/arrowsmith_uic/word_text_summary.html\n",
    "\n",
    "\n",
    "\n",
    "##################   file output for trials\n",
    "#outf = open(cachetest + 'trialdata' + job + '.tsv',\"w\",errors='ignore') \n",
    "\n",
    "#for t in alltrialdict.keys():\n",
    "#  #add the paragraph vectors to a text file\n",
    "#  feat = 'F13'\n",
    "#  vecchunk = alltrialdict[t][feat].replace('\\t','').replace('\\n','')\n",
    "#  _ = outf.write(t + '\\t' + feat + '\\t' + vecchunk + '\\n')\n",
    "\n",
    "#outf.close()\n",
    "\n",
    "\n",
    "\n",
    "##################### output title + abs for text similarity scoring\n",
    "#statmsg += '<b>Appplying <a href=\"http://arrowsmith.psych.uic.edu/arrowsmith_uic/word_text_summary.html\" target=\"_blank\">text similarity model</a> in the background.</b><br>'\n",
    "#outstat(statmsg,job)\n",
    "\n",
    "#outf = open(cachetest + 'pubdata' + job + '.tsv',\"w\",errors='ignore') \n",
    "\n",
    "#for ai, ar in article_rows.iterrows():\n",
    "#  ostr = ' '\n",
    "#  if ar['title'] is not None:\n",
    "#    ostr += ar['title']\n",
    "#  if ar['abstract'] is not None:\n",
    "#    ostr += ' ' + ar['abstract']\n",
    "#  vecchunk = ostr.replace('\\t','').replace('\\n','').strip()\n",
    "#  _ = outf.write(str(ar['PMID']) + '\\t' + vecchunk + '\\n')\n",
    "\n",
    "#outf.close()\n",
    "\n",
    "#tcmd = \"./simscore_batchjob.pl \"\n",
    "#if amtesting == 'Y':\n",
    "#  tcmd = \"./simscore_batchjobT.pl \"\n",
    "\n",
    "#process = subprocess.Popen([tcmd+job], shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, bufsize=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############################ related articles ranking\n",
    "\n",
    "statmsg += '<b>Retrieving related article rankings from PubMed.gov</b><BR>'\n",
    "outstat(statmsg,job)\n",
    "progress = '{0} article rankings retrieved out of ' + str(len(pmidlist)) \n",
    "\n",
    "#preload all of the related article rankings\n",
    "rankrack={}\n",
    "pc=0\n",
    "\n",
    "pset = [str(v) for v in pmidlist]\n",
    "pc = 0\n",
    "insdate = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "#function to repeatedly request data from PubMed\n",
    "def ranksTrying(pset):\n",
    "  tries=0\n",
    "  good=0\n",
    "  ubase = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi'\n",
    "  postobj = {'db':'pubmed','api_key':'','linkname':'pubmed_pubmed','retmode':'JSON','id':chunk}\n",
    "  racktemp={}\n",
    "  while good==0 and tries<10:\n",
    "    try:\n",
    "      x = requests.post(ubase,postobj)\n",
    "      xt = json.loads(x.text)\n",
    "      for xl in xt['linksets']:\n",
    "        p = xl['ids'][0]\n",
    "        ptemp = {}\n",
    "        x2 = xl['linksetdbs'][0]['links']\n",
    "        for i in range(0,len(x2)):\n",
    "          if i < 4000:\n",
    "            ptemp[x2[i]] = i +1\n",
    "        racktemp[int(p)] = ptemp\n",
    "      good=1\n",
    "    except:\n",
    "      tries+=1\n",
    "      time.sleep(5)\n",
    "  return racktemp\n",
    "\n",
    "\n",
    "\n",
    "#for chunk in chunker(pset,500):\n",
    "#  rt = ranksTrying(chunk)\n",
    "#  for r in rt.keys():\n",
    "#    rankrack[r] = rt[r]\n",
    "#  pc += len(chunk)\n",
    "#  outstat(statmsg + progress.format(str(pc)),job)\n",
    "\n",
    "\n",
    "with open('dataset_14.pkl', 'rb') as f:\n",
    "  rankrack = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################## Feature 02 authority table - faster lookup method with dictionaries\n",
    "statmsg += '<b>Retrieving <a href=\"http://arrowsmith.psych.uic.edu/arrowsmith_uic/author2.html\" target=\"_blank\">Author-ity</a> clusters.</b><br>'\n",
    "outstat(statmsg,job)\n",
    "progress = '{0} Author-ity clusters searched out of ' + str(len(pmidlist)) \n",
    "\n",
    "\n",
    "AuthorityRef = {}\n",
    "AuthorityDetails = {}\n",
    "pc=0\n",
    "#for i in pmidlist:\n",
    "#  selectSQL = 'select rowid, c1, ccount from PUBMED2015.Authorityclust where (MATCH(clist) AGAINST(\"' + str(i) + '_*\" IN BOOLEAN MODE)) ; '\n",
    "#  dbcursor.execute(selectSQL)\n",
    "#  table_rows = dbcursor.fetchall()\n",
    "#  #table_rows = []\n",
    "#  templist = []\n",
    "#  for row in table_rows:\n",
    "#    templist.append(row[0])\n",
    "#    if row[0] not in AuthorityDetails.keys():\n",
    "#      AuthorityDetails[row[0]] = row\n",
    "#  AuthorityRef[i] = templist\n",
    "#  pc+=1\n",
    "#  if pc%500 == 0:\n",
    "#    outstat(statmsg + progress.format(str(pc)),job)\n",
    "\n",
    "with open('dataset_16.pkl', 'rb') as f:\n",
    "  AuthorityRef = pickle.load(f)\n",
    "\n",
    "with open('dataset_17.pkl', 'rb') as f:\n",
    "  AuthorityDetails= pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "########################## Feature 02-09 - raw data table\n",
    "#finish processing the article_rows table\n",
    "pc=0\n",
    "#make all cap versions of title to speed up Feature 11\n",
    "article_rows['titleCaps'] = ''\n",
    "article_rows['titleCapsNoAdam'] = ''\n",
    "sw = set(s for s in stoplist)\n",
    "for index, row in article_rows.iterrows():\n",
    "  lv = set([suppressWords(s.strip()) for s in caps.findall(row['title'])])\n",
    "  lv.discard('remove')\n",
    "  article_rows.at[index,'titleCapsNoAdam'] = lv - sw\n",
    "  lv = set([s.strip() for s in caps.findall(row['title'])])\n",
    "  lv.discard('remove')\n",
    "  article_rows.at[index,'titleCaps'] = lv - sw\n",
    "  pc += 1\n",
    "  if pc % 500 == 0:\n",
    "    outstat(statmsg + progress.format(str(pc)),job)\n",
    "\n",
    "\n",
    "article_rows.set_index(['PMID'],inplace=True)\n",
    "\n",
    "article_rows['authors2'] = article_rows.apply(lambda row: expandAuth(row['authors']),axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################## Feature 10 - shared substances\n",
    "statmsg += '<b>Processing article chemicals and accession numbers.</b><br>'\n",
    "outstat(statmsg,job)\n",
    "\n",
    "\n",
    "chem_dict = {}\n",
    "\n",
    "idSQL = \"select group_concat(a.eirid separator ',') from pubmed.eir a where a.hs like '%Chemical/NameOfSubstance';\"\n",
    "#dbcursor.execute(idSQL)\n",
    "#ids = dbcursor.fetchall()\n",
    "#idst = ids[0][0]\n",
    "\n",
    "with open('dataset_18.pkl', 'rb') as f:\n",
    "  idst = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "selectSQL = \"\"\"SELECT a.aid, group_concat(a.val separator '|') as chemicals\n",
    "FROM pubmed.aelement a\n",
    "WHERE a.eirid in ({1})\n",
    "AND\n",
    "a.aid in ({0}) \n",
    "GROUP BY a.aid; \n",
    "\"\"\"\n",
    "\n",
    "#for chunk in chunker(pmidlist,500):\n",
    "#  dbcursor.execute(selectSQL.format(','.join([str(v) for v in chunk]),idst))     \n",
    "#  for r in dbcursor:\n",
    "#      chem_dict[r[0]] = r[1]\n",
    "\n",
    "\n",
    "with open('dataset_19.pkl', 'rb') as f:\n",
    "  chem_dict = pickle.load(f)\n",
    "\n",
    "\n",
    "foundlist = list(chem_dict.keys())\n",
    "for v in pmidlist:\n",
    "    if int(v) not in foundlist:\n",
    "      chem_dict[int(v)] = ''\n",
    "\n",
    "\n",
    "\n",
    "########################## Feature 10 - accession numbers\n",
    "acc_dict = {}\n",
    "\n",
    "idSQL = \"select group_concat(a.eirid separator ',') from pubmed.eir a where a.hs like '%/AccessionNumberList/AccessionNumber%';\"\n",
    "#dbcursor.execute(idSQL)\n",
    "#ids = dbcursor.fetchall()\n",
    "#idst = ids[0][0]\n",
    "\n",
    "with open('dataset_20.pkl', 'rb') as f:\n",
    "  idst = pickle.load(f)\n",
    "\n",
    "\n",
    "selectSQL = \"\"\"SELECT a.aid, group_concat(a.val separator '|') as accessionn\n",
    "FROM pubmed.aelement a\n",
    "WHERE a.eirid in ({1})\n",
    "AND\n",
    "a.aid in ({0}) \n",
    "GROUP BY a.aid; \n",
    "\"\"\"\n",
    "\n",
    "#for chunk in chunker(pmidlist,500):\n",
    "#  dbcursor.execute(selectSQL.format(','.join([str(v) for v in chunk]),idst))     \n",
    "#  for r in dbcursor:\n",
    "#      acc_dict[r[0]] = r[1]\n",
    "\n",
    "\n",
    "with open('dataset_21.pkl', 'rb') as f:\n",
    "  acc_dict = pickle.load(f)\n",
    "\n",
    "\n",
    "foundlist = list(acc_dict.keys())\n",
    "for v in pmidlist:\n",
    "    if int(v) not in foundlist:\n",
    "      acc_dict[int(v)] = ''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################## Feature 10 - collective names\n",
    "col_dict = {}\n",
    "\n",
    "idSQL = \"select group_concat(a.eirid separator ',') from pubmed.eir a where a.hs like '%CollectiveName%';\"\n",
    "#dbcursor.execute(idSQL)\n",
    "#ids = dbcursor.fetchall()\n",
    "#idst = ids[0][0]\n",
    "\n",
    "with open('dataset_22.pkl', 'rb') as f:\n",
    "  idst = pickle.load(f)\n",
    "\n",
    "\n",
    "selectSQL = \"\"\"SELECT a.aid, group_concat(a.val separator '|') as accessionn\n",
    "FROM pubmed.aelement a\n",
    "WHERE a.eirid in ({1})\n",
    "AND\n",
    "a.aid in ({0}) \n",
    "GROUP BY a.aid; \n",
    "\"\"\"\n",
    "\n",
    "#for chunk in chunker(pmidlist,500):\n",
    "#  dbcursor.execute(selectSQL.format(','.join([str(v) for v in chunk]),idst))     \n",
    "#  for r in dbcursor:\n",
    "#      col_dict[r[0]] = r[1]\n",
    "\n",
    "\n",
    "with open('dataset_23.pkl', 'rb') as f:\n",
    "  col_dict = pickle.load(f)\n",
    "\n",
    "\n",
    "foundlist = list(col_dict.keys())\n",
    "for v in pmidlist:\n",
    "    if int(v) not in foundlist:\n",
    "      col_dict[int(v)] = ''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################## Feature 12 - title cn all caps\n",
    "statmsg += '<b>Processing article titles and abstracts.</b><br>'\n",
    "outstat(statmsg,job)\n",
    "progress = '{0} titles and abstracts processed out of ' + str(len(pmidlist)) \n",
    "\n",
    "\n",
    "abs_rowdict={}\n",
    "abs_fulldict = {}\n",
    "pc=0\n",
    "\n",
    "\n",
    "\n",
    "#add a consolidated all caps set for title and abastracts\n",
    "article_rows['ta_allcaps'] = article_rows.apply(lambda row: set(), axis=1)\n",
    "\n",
    "for index, row in article_rows.iterrows():\n",
    "  if row['abstract'] is not None:\n",
    "    temp = set([suppressWords(s.strip()) for s in caps.findall(row['abstract'])])\n",
    "    temp.discard('remove')\n",
    "    abs_rowdict[index] = temp\n",
    "    abs_fulldict[index] = row['abstract']\n",
    "  else:\n",
    "    abs_rowdict[index] = set()\n",
    "    abs_fulldict[index] = ''\n",
    "  article_rows.at[index,'ta_allcaps'] = (row['titleCaps'] | abs_rowdict[index])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################# run text similarity model\n",
    "progress = '{0} text similarities calculated out of ' + str(len(pmidlist)) \n",
    "\n",
    "tnow = datetime.datetime.now()\n",
    "timelock = (tnow-basestart).total_seconds() \n",
    "tsimlen = max(1,len(pmidlist))\n",
    "\n",
    "\n",
    "#if process.poll() is None:\n",
    "#  while True:\n",
    "#    nextline = process.stdout.readline()\n",
    "#    if process.poll() is not None:\n",
    "#      break\n",
    "#    t=nextline.decode()\n",
    "#    pc = t.split(' ')[0]\n",
    "#    if int(pc) % 100 == 0:\n",
    "#      if esttime == 600:\n",
    "#        esttime = timelock + (tsimlen-int(pc))*0.11\n",
    "#      outstat(statmsg + progress.format(pc),job)\n",
    "#    sys.stdout.flush()\n",
    "#  output = process.communicate()[0]\n",
    "#  exitCode = process.returncode\n",
    "#else:\n",
    "#  statmsg += 'Text similarity complete.'\n",
    "#  outstat(statmsg ,job)\n",
    "\n",
    "#simdat = pd.read_csv(cachetest + 'simout'+job+'.tsv',sep='\\t',header=None)\n",
    "#simdat.columns = ['PMID','NCT_num','feat','result']\n",
    "\n",
    "with open('dataset_24.pkl', 'rb') as f:\n",
    "  simdat = pickle.load(f)\n",
    "\n",
    "\n",
    "#using a dictionary here with concatenated keys for speed during model application\n",
    "simdict = {}\n",
    "for index, row in simdat.iterrows():\n",
    "  simdict[str(row['PMID'])+':'+row['NCT_num']+':'+row['feat']] = decode_sims(row['result'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################################\n",
    "# model application loop\n",
    "#################################################\n",
    "mod2 = {}\n",
    "\n",
    "tttu = time.time()\n",
    "\n",
    "timedict = {}\n",
    "timedict['pmiddat'] = []\n",
    "timedict['ctvalues'] = []\n",
    "\n",
    "progress = '{0} model scores computed out of ' + str(len(pmidlist))\n",
    "\n",
    "\n",
    "for mainindex,mainrow in nset.iterrows():\n",
    "  ttu = time.time()\n",
    "  trialdict =  alltrialdict[mainrow['NCT_num']]\n",
    "  timedict['ctvalues'].append(((time.time()-ttu)*1000))\n",
    "  newrow = modelform.copy()\n",
    "  newrow.at[0,'sample_id'] = 0\n",
    "  newrow.at[0,'NCT_num'] = mainrow['NCT_num']\n",
    "  newrow.at[0,'PMID'] = mainrow['PMID']\n",
    "  newrow.at[0,'posneg'] = 0\n",
    "  newrow.at[0,'testcontrol'] = 0\n",
    "  ttu = time.time()\n",
    "  pdat = {}\n",
    "  pdat['title'] = article_rows.at[mainrow['PMID'],'title']\n",
    "  pdat['mesh'] = article_rows.at[mainrow['PMID'],'mesh']\n",
    "  pdat['grants'] = article_rows.at[mainrow['PMID'],'grants']\n",
    "  pdat['email'] = article_rows.at[mainrow['PMID'],'email']\n",
    "  pdat['chemicals'] = chem_dict[mainrow['PMID']]\n",
    "  pdat['authors'] = article_rows.at[mainrow['PMID'],'authors']\n",
    "  pdat['accession'] = acc_dict[mainrow['PMID']]\n",
    "  pdat['affiliation'] = article_rows.at[mainrow['PMID'],'affiliation']\n",
    "  pdat['abstract'] = abs_fulldict[mainrow['PMID']]\n",
    "  pdat['cn'] = col_dict[mainrow['PMID']]\n",
    "  pdat['pmid'] = str(mainrow['PMID'])\n",
    "  pdat['titleabs'] = pdat['title'] + ' ' + pdat['abstract']\n",
    "  pdat['ta_allcaps'] = article_rows.at[mainrow['PMID'],'ta_allcaps']\n",
    "  timedict['pmiddat'].append(((time.time()-ttu)*1000))\n",
    "  for index, row in flistdf.iterrows():\n",
    "    ctvalue = trialdict[row['featnum']]\n",
    "    for p in row['pubmedlist']:\n",
    "      for m in row['methodlist']:\n",
    "        ttu = time.time()\n",
    "        if m in ['tsim','rksimax']:\n",
    "          simkey = str(mainrow['PMID'])+':'+mainrow['NCT_num']+':'+row['featnum']\n",
    "          if simkey in simdict.keys():\n",
    "            compdict = simdict[simkey]\n",
    "            newrow.at[0,row['featnum']+'_'+p+'_'+m+'unw_tms'] = compdict['unw_tms']\n",
    "            newrow.at[0,row['featnum']+'_'+p+'_'+m+'unw_scr'] = compdict['unw_scr']\n",
    "            newrow.at[0,row['featnum']+'_'+p+'_'+m+'w_tms'] = compdict['w_tms']\n",
    "            newrow.at[0,row['featnum']+'_'+p+'_'+m+'w_scr'] = compdict['w_scr']\n",
    "          else:\n",
    "            newrow.at[0,row['featnum']+'_'+p+'_'+m+'unw_tms'] = np.nan\n",
    "            newrow.at[0,row['featnum']+'_'+p+'_'+m+'unw_scr'] = np.nan\n",
    "            newrow.at[0,row['featnum']+'_'+p+'_'+m+'w_tms'] = np.nan\n",
    "            newrow.at[0,row['featnum']+'_'+p+'_'+m+'w_scr'] = np.nan\n",
    "        else:\n",
    "          pmvalue = pdat[p]\n",
    "          pmObj = getattr(sys.modules[__name__], 'comp_' + m )\n",
    "          if (len(ctvalue) < 2 or len(pmvalue) < 2) and m != 'mismatch':\n",
    "            compval = np.nan\n",
    "          else:\n",
    "            if m == 'mismatch':\n",
    "              compval = pmObj(ctvalue,pmvalue)\n",
    "            else:\n",
    "              compval = pmObj(ctvalue.encode('ascii',errors='ignore').decode('ascii'),pmvalue)\n",
    "          if m in ['occurance','propoccurance']:\n",
    "            if np.isnan(compval):\n",
    "              compval=0\n",
    "            else:\n",
    "              if compval > 0:\n",
    "                compval = .5\n",
    "              else:\n",
    "                compval = -.5\n",
    "          newrow.at[0,row['featnum']+'_'+p+'_'+m] = compval\n",
    "        if m not in timedict.keys():\n",
    "            timedict[m] = [((time.time()-ttu)*1000)]\n",
    "        else:\n",
    "            timedict[m].append(((time.time()-ttu)*1000))\n",
    "        #print(datetime.datetime.now())\n",
    "  if mainrow['trial_start'] is None:\n",
    "    newrow.at[0,'startdate_lift'] = 0\n",
    "    newrow.at[0,'start_diff_lt5'] = 0\n",
    "    newrow.at[0,'start_diff_cast'] = 0\n",
    "  else:\n",
    "    dt = np.floor((mainrow['pubdate']-mainrow['trial_start']).days/365)\n",
    "    if dt > -1 and dt < 17:\n",
    "      newrow.at[0,'startdate_lift'] = startlift[dt]\n",
    "    else:\n",
    "      if dt < 0:\n",
    "        newrow.at[0,'startdate_lift'] = 0\n",
    "      else:\n",
    "        newrow.at[0,'startdate_lift'] = 1\n",
    "    if dt < 5 and dt> -1:\n",
    "      newrow.at[0,'start_diff_lt5'] = 1\n",
    "    else:\n",
    "      newrow.at[0,'start_diff_lt5'] = 0\n",
    "    newrow.at[0,'start_diff_cast'] = 0\n",
    "  atemp = max(np.nan_to_num([newrow.at[0,'F01_authors_authtree'],newrow.at[0,'F31_authors_authtree'],newrow.at[0,'F36_authors_authtree']]))\n",
    "  newrow.at[0,'authcomp'] = atemp\n",
    "  arounded = round(min(900,max(0,atemp))/100,0)\n",
    "  newrow.at[0,'agginvint'] = max(aggdict[arounded],newrow.at[0,'F03_pmid_aggmax'])\n",
    "  score = float(MOD_intercept)\n",
    "  for v in MOD.keys():\n",
    "    #print(v,newrow.at[0,v])\n",
    "    if pd.isnull(newrow.at[0,v]):\n",
    "      newrow.at[0,v] = MODsub[v]\n",
    "      score += MODsub[v]*MOD[v]\n",
    "    else:\n",
    "      score += float(newrow.at[0,v])*MOD[v]\n",
    "  newrow.at[0,'prob'] =(np.exp(score) / (1 + np.exp(score)))\n",
    "  if len(searchlinks)==1 and mainrow['PMID'] in searchlinks:\n",
    "    newrow.at[0,'prob'] = 1\n",
    "  #modeldata = modeldata.append(newrow)\n",
    "  t=newrow.values.tolist()\n",
    "  mod2[mainrow['NCT_num'] + ':'+str(mainrow['PMID'])] = t[0]\n",
    "  if mainindex%500 == 0:\n",
    "    #modeldata.to_csv('scoringpartial.csv')\n",
    "    outstat(statmsg + progress.format(str(mainindex)),job)\n",
    "    #print(mainindex,len(nset))\n",
    "\n",
    "\n",
    "\n",
    "modeldata = pd.DataFrame.from_dict(mod2,orient='index',columns=newrow.columns)\n",
    "\n",
    "\n",
    "#mark articles if they are NCT linked or in the Trial Results\n",
    "modeldata['Trial Results'] = modeldata.apply(lambda row: 'yes' if row['PMID'] in searchres else 'no', axis=1)\n",
    "modeldata['NCT Link'] = modeldata.apply(lambda row: 'yes' if row['PMID'] in searchlinks else 'no', axis=1)\n",
    "modeldata['rank'] = -1\n",
    "i=0\n",
    "\n",
    "#make rank integers\n",
    "for index,row in modeldata.sort_values(['prob'],ascending=False).iterrows():\n",
    "  i+=1\n",
    "  modeldata.at[index,'rank'] = i\n",
    "\n",
    "#filter by rank or link status\n",
    "modeldata['keep'] = modeldata.apply(lambda row: 1 if row['rank']<11 or row['prob'] > 0.7999 else 0, axis=1)\n",
    "modeldata['keep'] = modeldata.apply(lambda row: 1 if row['Trial Results']=='yes' or row['NCT Link'] == 'yes' else row['keep'], axis=1)\n",
    "\n",
    "\n",
    "modeldata.sort_values(['prob'],ascending=[False]).to_csv(f\"scoreout_{trial}_data.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NCT03745053': {'F09': 'Interventional',\n",
       "  'F36': '',\n",
       "  'F30': 'XLIMIT',\n",
       "  'F33': '',\n",
       "  'F01': 'Luca Testa, MD',\n",
       "  'F31': '',\n",
       "  'F16': 'Xlimus DES Implantation during coronary angioplasty|Synergy DES Implantation during coronary angioplasty',\n",
       "  'F27': 'XLIMIT',\n",
       "  'F02': 'XLIMus Drug Eluting Stent: a randomIzed Controlled Trial to Assess Endothelization',\n",
       "  'F10': 'Randomized',\n",
       "  'F15': 'Coronary Artery Disease',\n",
       "  'F25': 'Coronary Artery Disease',\n",
       "  'F29': 'Italy|Spain',\n",
       "  'F24': '',\n",
       "  'F13': 'The objective of the study is to assess angiographic and clinical performance of Xlimus Drug      Eluting Stent (DES) compared to Synergy Bioabsorbable Polymer Everolimus Eluting Stent in      patients treated with percutaneous coronary angioplasty',\n",
       "  'FTA': {'XLIMIT'},\n",
       "  'F03': 'NCT03745053'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alltrialdict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f19269c3532963e6f67d1576f65b9658be42cea15b54c7b0db67021a36cbeb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
